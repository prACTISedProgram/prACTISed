#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#20220906 JL    Note: This code was modified from Sven Kochmann's 2019 Knuteon

# Knuteon!
# Copyright (C) 2019 by Sven Kochmann

# Program name was generated by chem-name-gen, see 
# DOI: 10.5281/zenodo.2578429

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the  Free Software Foundation,  either version 3  of the License, or
# (at your option) any later version.

# This program  is distributed  in the hope  that it will  be  useful,
# but  WITHOUT  ANY  WARRANTY;  without even  the implied warranty  of
# MERCHANTABILITY  or  FITNESS  FOR  A  PARTICULAR  PURPOSE.  See  the
# GNU General Public License for more details.

# You  should  have  received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

# This program extracts the data lines from Karat32 data files.  These
# data files are simple OLE files with the following structure:
# 
# experiment.dat
#       |-> AuditTrail
#       |-> Baseline Check
#       |-> ChangeLog
#       |-> Detector Data          <-- the folder with the detector traces
#       |-> MethodXX
#       |-> Results
#       |-> Signature
#       |-> XY Data
#       [5]SummaryInformation
#       3D Data
#       3D Trace Handler
#       Chrom Header                   <-- header file containing general info
#       Contents                                   <-- list of traces
#       Detector Trace Handler
#       Extra Sequence Columns
#       FileValidation
#       Multichromatogram Info
#       ResultEntries
#       XY Trace Handler

# Imports
import argparse
import datetime
import math
import olefile
import os
import pathlib
import practised_pwexplode
import signal
import struct
import pandas as pd

# This fixes Python's default behaviour  of throwing an exception when
# pipe reading (e.g. with 'head')  suddenly stops. Who though this was
# a good default behaviour anyway?  This will  not work under Windows.
if os.name is 'posix':
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)


######################################################################
# Extracts  the actual trace data  and  returns it as  list of tuples.
# This function needs  the header information gotten by "read_traces".
# Data  might  actually be compressed by PKWARE compression,  which is
# handled by the pwexplode library. 
def extract_trace(ole, header):
        datapoints = []
        datastream = ole.openstream(['Detector Data', 'Detector %s Trace' % header['id']])

        # The stream starts  with 5 integers:  version,  number of points,
        # maximum points, channels, compression flag. Right now, we ignore
        # everything except the number of points and the compression flag.
        version = struct.unpack('<I', datastream.read(4))[0]
        nopoints = struct.unpack('<I', datastream.read(4))[0]
        maxpoints = struct.unpack('<I', datastream.read(4))[0] 
        channels = struct.unpack('<I', datastream.read(4))[0]
        compression = struct.unpack('<I', datastream.read(4))[0]
        
        decompresseddata = ""
        # Compression is 1  if PWWARE compression  was used  for the data.
        # Other compression methods are not known or not used.
        if compression == 1:
                # The data is chunked into blocks of 2048. As soon as we read
                # a block of less, we have all the data! 
                readablock = True
                compresseddata = b""
                while readablock:
                        length = struct.unpack('<H', datastream.read(2))[0]
                        compresseddata += datastream.read(length)
                        
                        if not length == 2048:
                                readablock = False

                # Decompress all the data
                decompresseddata = practised_pwexplode.explode(compresseddata)
        else:
                # Each data point is an integer of 4 bytes, read all in here
                decompresseddata = datastream.read(nopoints * 4)

        # Convert  the list of integers  into (time, signal)-tuples.  Time 
        # is given by  the index of  the data point  and  the sample rate,
        # while  the actual signal is given by  the data point integer and
        # the multiplier. Note, that the time is actually saved in seconds
        # but always presented as minutes. We keep seconds here.
        pos = 0
        for i in range(nopoints):
                datapoint = struct.unpack('<i', decompresseddata[pos:pos + 4])[0]
                pos += 4

                time = i / header['sample rate']
                signal = datapoint * header['multiplier']
                
                datapoints.append((time, signal))

        return datapoints


######################################################################
# Reads all traces  in the given data file  and returns them as a list
# of dictionaries
def read_traces(ole):
        # A blank trace dictionary
        trace_dict = {'id': '', 'y-axis name': '', 'y-axis name (internal)': '', 'y-axis unit': '', 'y-axis unit (internal)': '',
                                  'sample rate': 0.0, 'multiplier': 1.0, 'x-axis name': '', 'x-axis name (internal)': '', 'maximum time': 0.0}
        traces = []

        # Traces are all saved in 'Detector Trace Handler',  so let's read
        # it and collect the information in there
        data_dth = ole.openstream('Detector Trace Handler')

        # Unfortunately, the first 20 Bytes are unknown, just skip
        data_dth.read(20)

        # 2 Bytes (Word) give  the number of signals followed by 4 unknown
        # bytes (that are usually FFFF0000), which we skip
        nosignals = struct.unpack('<H', data_dth.read(2))[0]
        data_dth.read(4)

        # Next up  is  an identifier string (length + string itself)  that
        # should always be 'CDetTracInfo'.  This is a soft check for this,
        # i.e. if it is NOT we just print a warning but go on!
        length = struct.unpack('<H', data_dth.read(2))[0]
        identifier = data_dth.read(length).decode()
        if identifier != "CDetTraceInfo":
                print("Warning: 'Detector Trace Handler' is not of type 'CDetTraceInfo'")
        
        # Now, there is a block for each signal
        for i in range(nosignals):
                # Create a COPY of the empty dictionary!
                tracedata = trace_dict.copy()

                # 4 Bytes are unknown, just skip
                data_dth.read(4)

                # These 4 bytes give some sort of  type number, followed again
                # by 4 unknown bytes, which we skip
                typeno = struct.unpack('<I', data_dth.read(4))[0]
                data_dth.read(4)

                # There are two known types so far: 8 and 9, which just define
                # how the detector trace ID is encoded:
                # Type 8: trace ID = integer
                # Type 9: trace ID = string
                if typeno == 8:
                        tracedata['id'] = str(struct.unpack('<I', data_dth.read(4))[0])
                elif typeno == 9:
                        length = struct.unpack('B', data_dth.read(1))[0]
                        tracedata['id'] = data_dth.read(length).decode()
                else:
                        print("Warning: type %d unknown for signal trace %d" % (typeno, i+1))

                # 4 Bytes are unknown, just skip
                data_dth.read(4)

                # Next bytes encode signal name (y-axis), sample rate (in Hz), 
                # unit (y-axis), multiplier, x-axis name, maximum time, y-axis
                # name  (internal),   y-axis  unit  (internal),   x-axis  name 
                # (internal)
                length = struct.unpack('B', data_dth.read(1))[0]
                tracedata['y-axis name'] = data_dth.read(length).decode()

                tracedata['sample rate'] = (1.0/float(struct.unpack('<f', data_dth.read(4))[0]))

                length = struct.unpack('B', data_dth.read(1))[0]
                tracedata['y-axis unit'] = data_dth.read(length).decode('cp1252', errors = 'ignore')

                tracedata['multiplier'] = float(struct.unpack('<f', data_dth.read(4))[0])

                # Technically,  this will usually lead to  'Minutes'.  However 
                # the actual data is saved and decoded in seconds.  Therefore,
                # we replace it here by the more accurate 'Time (s)'.
                length = struct.unpack('B', data_dth.read(1))[0]
                data_dth.read(length)
                tracedata['x-axis name'] = "Time (s)"
                
                # These  are  two floats (4 bytes each).  The first one is not
                # known, so we just skip it for now.  It could be the steps in
                # minutes  for  each  data point  acquired  (redundant  sample 
                # rate).  The second one is  the maximum time (in seconds) set  
                # by  the user  when setting up  the method.  But it  does not 
                # mean this time was reached!  Indeed,  the user can stop  the 
                # measurement at any time giving less  data points in the end.
                # These floats are followed by 24 unknown bytes, which we just 
                # ignore.
                struct.unpack('<f', data_dth.read(4))[0]
                tracedata['maximum time'] = struct.unpack('<f', data_dth.read(4))[0]
                data_dth.read(24)

                length = struct.unpack('B', data_dth.read(1))[0]
                tracedata['y-axis name (internal)'] = data_dth.read(length).decode()

                length = struct.unpack('B', data_dth.read(1))[0]
                tracedata['y-axis unit (internal)'] = data_dth.read(length).decode('cp1252', errors = 'ignore')

                # 8 Bytes are unknown, just skip
                data_dth.read(8)

                length = struct.unpack('B', data_dth.read(1))[0]
                tracedata['x-axis name (internal)'] = data_dth.read(length).decode()

                # Last 26 bytes are unknown, skip and go one with next trace
                data_dth.read(26)

                # Add to output
                traces.append(tracedata)                

        return traces


######################################################################
# Converts an OLE variant time double to system time and returns it as 
# a datetime object. Math based on the description at
# https://docs.microsoft.com/en-us/dotnet/api/system.datetime.tooadate
def variant_to_system_time(oletime):
        # Date is before the period, the time afterwards; we have to check 
        # for negative times/dates; we add 1e-11 to force double precision
        # for the time part
        datepart = (math.ceil(oletime) if oletime < 0.0 else math.floor(oletime))
        timepart = math.fabs(oletime - datepart + 1e-11)

        if timepart >= 1.0:
                timepart -= 1e-11

        # Convert date part to days, months, and years; the reference date
        # for OLE variant times is December 30, 1899.
        realdate = datetime.datetime(1899, 12, 30) + datetime.timedelta(days=int(datepart))     

        # Time  is  represented  as  fraction  of 24 hours  with .00 being 
        # 0:00 am, .25 being 6:00 am, etc. We just multiply     the value with 
        # 24 hours á 60 minutes á 60 seconds  and add it to the date.
        realdate = realdate + datetime.timedelta(seconds=int(timepart * 24 * 3600))

        return realdate


######################################################################
# Reads 'chrom header' from the OLE data file and returns its contents
# as a dictionary
def read_chrom_header(ole):
        headerinfo = {'runtime': datetime.datetime(1899, 12, 30), 'method name': '', 'method path': '', 'description': '', 
                                  'version': '', 'system': '', 'detector': ''}

        chromhead = ole.openstream('Chrom Header')

        # First 8 bytes of this file are unknown, just read to skip
        chromhead.read(8)

        # Next 8 bytes encode the runtime as a double (OLE variant date)
        headerinfo['runtime'] = struct.unpack('<d', chromhead.read(8))[0]
        headerinfo['runtime'] = variant_to_system_time(headerinfo['runtime'])

        # Next  bytes  contain  the strings  for  the method  file,  data
        # description,  version info,  and system info  each encoded as a 
        # C-string (length as a byte + the string itself, max 255 bytes)
        length = struct.unpack('B', chromhead.read(1))[0]
        method = pathlib.PureWindowsPath(chromhead.read(length).decode())
        headerinfo['method name'] = method.name
        headerinfo['method path'] =     method.parent

        length = struct.unpack('B', chromhead.read(1))[0]
        headerinfo['description'] = chromhead.read(length).decode()

        length = struct.unpack('B', chromhead.read(1))[0]
        headerinfo['version'] = chromhead.read(length).decode()

        length = struct.unpack('B', chromhead.read(1))[0]
        headerinfo['system'] = chromhead.read(length).decode()

        # Next 22 bytes are unknown, just read to skip
        chromhead.read(22)

        # Detector string encoded as C-String (see above)
        length = struct.unpack('B', chromhead.read(1))[0]
        headerinfo['detector'] = chromhead.read(length).decode()

        return headerinfo


######################################################################
# Main program of Knuteon!
def readTrace(inputFile):
        # Opening of file, reading of header information
        if olefile.isOleFile(inputFile) == False:
                print("%s is not an ole compound file, so it cannot be a Karat32 data file. Sorry." % args['inputfile'][0])
                exit()

        ole = olefile.OleFileIO(inputFile)
        basename = os.path.splitext(inputFile)[0]

        header = read_chrom_header(ole)
        header.update({'filename': basename})

        traces = read_traces(ole)
        for trace in traces:
                if trace['id']== str(0):
                        traceheader = trace
                        break

        data = extract_trace(ole, traceheader)
        new = True
        run = []
        for point in data:
                run.append([point[0], point[1]])

        run = pd.DataFrame(run)
        return run

